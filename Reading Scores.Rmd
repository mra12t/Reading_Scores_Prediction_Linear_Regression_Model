---
title: "Reading Scores Prediction"
output: github_document
date: "2022-12-15"
---

# Overview
The Programme for International Student Assessment (PISA) is a test that is administered every three years to 15-year-old students from around the world. The test evaluates students' performance in mathematics, reading, and science and provides a quantitative way to compare the performance of students from different regions. In this homework assignment, we will use the pisa2009train.csv and pisa2009test.csv datasets to predict the reading scores of American students who took the PISA exam in 2009. These datasets contain information about the demographics and schools of the students, but are not supposed to include any identifying information. By using these datasets, you are agreeing to the NCES data use agreement, which prohibits attempting to determine the identity of any student in the datasets.

# About the Data Set
The pisa2009train.csv and pisa2009test.csv data sets contain information about students who took the PISA exam in 2009. Each row in the dataset represents one student, and the datasets include a variety of variables that provide information about the student's background, school, and performance on the exam. Some of the variables in the datasets include:

grade: The grade in school of the student (most 15-year-olds in America are in 10th grade)
male: Whether the student is male (1/0)
raceeth: The race/ethnicity composite of the student
preschool: Whether the student attended preschool (1/0)
expectBachelors: Whether the student expects to obtain a bachelor’s degree (1/0)
motherHS: Whether the student’s mother completed high school (1/0)
motherBachelors: Whether the student’s mother obtained a bachelor’s degree (1/0)
motherWork: Whether the student’s mother has part-time or full-time work (1/0)
fatherHS: Whether the student’s father completed high school (1/0)
fatherBachelors: Whether the student’s father obtained a bachelor’s degree (1/0)
fatherWork: Whether the student’s father has part-time or full-time work (1/0)
selfBornUS: Whether the student was born in the United States of America (1/0)
motherBornUS: Whether the student’s mother was born in the United States of America (1/0)
fatherBornUS: Whether the student’s father was born in the United States of America (1/0)
englishAtHome: Whether the student speaks English at home (1/0)
computerForSchoolwork: Whether the student has access to a computer for schoolwork (1/0)
read30MinsADay: Whether the student reads for pleasure for 30 minutes/day (1/0)
minutesPerWeekEnglish: The number of minutes per week the student spend in English class
studentsInEnglish: The number of students in this student’s English class at school
schoolHasLibrary: Whether this student’s school has a library (1/0)
publicSchool: Whether this student attends a public school (1/0)
urban: Whether this student’s school is in an urban area (1/0)
schoolSize: The number of students in this student’s school
readingScore: The student’s reading score, on a 1000-point scale

# EDA
```{r}
# Loading the Data Sets 
pisaTrain = read.csv("pisa2009train.csv")
pisaTest = read.csv("pisa2009test.csv")

# Exploring the number of Students in the training set
nrow(pisaTrain)

```
As we can see the number of the students in the training set is 3663.   
Now lets explore the data a bit
```{r}
library(knitr)
#the avreage reading test score of males and females in the Training set
z = tapply(pisaTrain$readingScore, pisaTrain$male, mean)
kable(z)

```
As we can see the avreage reading score of males is 483.5325 and the avreage reading score of females is 512.9406.  
Now lets look if there's any missing value in our data set. 
```{r}
kable(summary(pisaTrain))
```
From the table above we can see which variables have how many missing values (NA).  
Now, let's erase the records with missing values
```{r}
pisaTrain = na.omit(pisaTrain)
pisaTest = na.omit(pisaTest)
nrow(pisaTrain)
```
As we can see the records had dropped from 3663 to 2414. Meaning we have more than 30% non-complete record which can be explained by the fact that these surveys are not completly filled most of the time by the survey takers. 

### Factor Variables 
Factor variables are variables that can take on a specific set of values. For example, a "Region" variable is a factor variable because it can only take on a certain number of values, such as "North America" or "Africa". These values are called levels.

A factor variable can either be unordered or ordered. An unordered factor, like the "Region" variable, does not have a natural ordering between its levels. This means that the levels can be arranged in any order and the variable will still have the same meaning. An ordered factor, on the other hand, has a natural ordering between its levels. For example, the classifications "large," "medium," and "small" are ordered because there is a clear hierarchy between the levels (large is greater than medium, which is greater than small). This means that the levels must be arranged in a specific order for the variable to have meaning.

In a linear regression model, unordered factor variables need to be encoded in a specific way to be used as predictors. One approach is to define one level of the factor as the reference level and add a binary variable for each of the remaining levels. This means that a factor with n levels is replaced by n-1 binary variables. The reference level is typically chosen to be the most frequently occurring level in the dataset.

For example, suppose we have an unordered factor variable called "color" with levels "red", "green", and "blue". If "green" is selected as the reference level, we would add binary variables "colorred" and "colorblue" to the linear regression model. For each red example, "colorred" would be set to 1 and "colorblue" would be set to 0. For each blue example, "colorred" would be set to 0 and "colorblue" would be set to 1. For each green example, both "colorred" and "colorblue" would be set to 0.

In the data set, the variable "raceeth" has levels "American Indian/Alaska Native", "Asian", "Black", "Hispanic", "More than one race", "Native Hawaiian/Other Pacific Islander", and "White". Because "White" is the most common level in our population, we would select it as the reference level and encode the remaining levels as binary variables. This would allow us to include "raceeth" as a predictor in our linear regression model.

Basically, we are creating a binary variable for each level except for the refernce level. In this case, for the variable raceeth, all but White. Meaning, a student who is Asian will have a value of 1 for the raceethAsian variable, and all other raceeth binary variables will have a value of 0. In contrast, a white student will have a value of 0 for all raceeth binary variables, since "White" is the reference level in this context.  

# Building a Model
```{r}
#Converting the raceeth variable into factor in the data sets
pisaTrain$raceeth = as.factor(pisaTrain$raceeth)
pisaTest$raceeth = as.factor(pisaTest$raceeth)

#Ordering the factors with reference to "White"
pisaTrain$raceeth = relevel(pisaTrain$raceeth, "White")
pisaTest$raceeth = relevel(pisaTest$raceeth, "White")

# Training a Linear Regression Model
lmodel = lm(readingScore ~. , data = pisaTrain)
summary(lmodel)
```
### Calculating RMSE
```{r}
SSE = sum(lmodel$residuals^2)
RMSE = sqrt(SSE/nrow(pisaTrain))
RMSE
```
The RMSE is 73.37 

### Understanding the Model

* In the model, two students that are on different grades but have all the other variables as identical will have a different reading score by $(number of grades)*(Factor of grades variable)$ Which in this case happen to be 29.54. 
* The Model is to expect that if two students have all the variable identical except for the race of the first being Asian and the second is White. The former will get lower score by a factor of -4.11.

# Applying the model on the Test set

```{r}
predTest = predict(lmodel, newdata = pisaTest)
```

### Let's see how good it is
```{r}
SSE = sum((predTest - pisaTest$readingScore)^2)
RMSE1 = sqrt(mean((predTest-pisaTest$readingScore)^2))
RMSE1
#BaseLine model RMSE
baseline = mean(pisaTest$readingScore)
RMSE2 = sqrt(mean((baseline-pisaTest$readingScore)^2))
RMSE2
```
While not much sophisticated, this simple linear model has lower RMSE than a base model. 

